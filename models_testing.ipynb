{"cells":[{"cell_type":"code","execution_count":1,"id":"modern-night","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"modern-night","executionInfo":{"status":"ok","timestamp":1699290491333,"user_tz":360,"elapsed":24143,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"5bc9da8a-34e1-452a-b996-da603ac6d3b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu118\n","Collecting traitlets==4.3.3\n","  Downloading traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipython-genutils (from traitlets==4.3.3)\n","  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n","Collecting six (from traitlets==4.3.3)\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting decorator (from traitlets==4.3.3)\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Installing collected packages: ipython-genutils, six, decorator, traitlets\n","  Attempting uninstall: ipython-genutils\n","    Found existing installation: ipython-genutils 0.2.0\n","    Uninstalling ipython-genutils-0.2.0:\n","      Successfully uninstalled ipython-genutils-0.2.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.7.1\n","    Uninstalling traitlets-5.7.1:\n","      Successfully uninstalled traitlets-5.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","jupyter-core 5.4.0 requires traitlets>=5.3, but you have traitlets 4.3.3 which is incompatible.\n","jupyter-server 1.24.0 requires traitlets>=5.1, but you have traitlets 4.3.3 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","nbclient 0.8.0 requires traitlets>=5.4, but you have traitlets 4.3.3 which is incompatible.\n","nbconvert 6.5.4 requires traitlets>=5.0, but you have traitlets 4.3.3 which is incompatible.\n","nbformat 5.9.2 requires traitlets>=5.1, but you have traitlets 4.3.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed decorator-5.1.1 ipython-genutils-0.2.0 six-1.16.0 traitlets-4.3.3\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==228 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==228\u001b[0m\u001b[31m\n","\u001b[0mTrue\n"]}],"source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(torch.__version__)\n","\n","\n","!python -m pip install traitlets==4.3.3 --force-reinstall\n","!pip install pywin32==228\n","\n","print(torch.cuda.is_available())\n","\n"]},{"cell_type":"markdown","id":"known-throw","metadata":{"id":"known-throw"},"source":["## Importing Training Text (can be changed later on)"]},{"cell_type":"code","execution_count":2,"id":"rational-beach","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"rational-beach","executionInfo":{"status":"ok","timestamp":1699290512167,"user_tz":360,"elapsed":20841,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"004d97ac-c9ee-48d3-a424-c2cf75eedeb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["'sed by many more; for if you succeed in this you will have achieved no small success.” In profound silence I listened to what my friend said, and his observations made such an impression on me that, without attempting to question them, I admitted their soundness, and out of them I determined to make this Preface; wherein, gentle reader, thou wilt perceive my friend’s good sense, my good fortune in finding such an adviser in such a time of need, and what thou hast gained in receiving, without addition or alteration, the story of the famous Don Quixote of La Mancha, who is held by all the inhabitants of the district of the Campo de Montiel to have been the chastest lover and the bravest knight that has for many years been seen in that neighbourhood. I have no desire to magnify the service I render thee in making thee acquainted with so renowned and honoured a knight, but I do desire thy thanks for the acquaintance thou wilt make with the famous Sancho Panza, his squire, in whom, to my th'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","with open('/content/drive/MyDrive/LLM TESTING/NanoGPT/data/don_quixote.txt', \"r\", encoding = 'utf-8') as f:\n","    text = f.read()\n","\n","text= text.replace('\\n', ' ').replace('  ', ' ')[121506:]\n","text[:1000]"]},{"cell_type":"code","execution_count":3,"id":"green-diversity","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"green-diversity","executionInfo":{"status":"ok","timestamp":1699290512167,"user_tz":360,"elapsed":13,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"3c3be56b-d887-4976-b4e5-cc971dd4b34e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Words on .txt file:  2190617\n","Unique characters:  100\n"," !$%&()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyzÁÆÑÚàáæéëíñóùŒœ—‘’“”•™\n","\n"]}],"source":["print('Words on .txt file: ', len(text))\n","\n","chars = sorted(list(set(text)))\n","char_vocab_size = len(chars)\n","\n","print('Unique characters: ', char_vocab_size)\n","print(''.join(chars))\n","print()"]},{"cell_type":"code","execution_count":3,"id":"structured-shark","metadata":{"id":"structured-shark","executionInfo":{"status":"ok","timestamp":1699290512168,"user_tz":360,"elapsed":11,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"particular-apparel","metadata":{"id":"particular-apparel"},"source":["## Using TikToken For Tokenization (can be changed later on to other like SentencePiece)\n","Tiktoken does tokenization per word and special character."]},{"cell_type":"code","execution_count":4,"id":"outstanding-skirt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"outstanding-skirt","executionInfo":{"status":"ok","timestamp":1699290519252,"user_tz":360,"elapsed":7094,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"9ddf3e25-36a1-4b4c-9a9a-18cda141b1cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n","Installing collected packages: tiktoken\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tiktoken-0.5.1\n"]}],"source":["!pip install tiktoken"]},{"cell_type":"code","execution_count":5,"id":"photographic-exception","metadata":{"id":"photographic-exception","executionInfo":{"status":"ok","timestamp":1699290521618,"user_tz":360,"elapsed":2383,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["import tiktoken\n","enc = tiktoken.get_encoding('gpt2')"]},{"cell_type":"code","execution_count":6,"id":"engaging-evans","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"engaging-evans","executionInfo":{"status":"ok","timestamp":1699290521618,"user_tz":360,"elapsed":8,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"10564dd7-b2d4-42c7-c023-acbe7a5c7924"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tonkenized 'testing':  [33407]\n","testing\n"]}],"source":["#Testing encoder\n","encoded_test_str= enc.encode('testing')\n","print(\"Tonkenized 'testing': \", encoded_test_str)\n","\n","decoded_test_str= enc.decode(encoded_test_str)\n","print(decoded_test_str)"]},{"cell_type":"markdown","id":"developed-insured","metadata":{"id":"developed-insured"},"source":["## Preparing Training Data"]},{"cell_type":"code","execution_count":7,"id":"broad-charge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"broad-charge","executionInfo":{"status":"ok","timestamp":1699290522270,"user_tz":360,"elapsed":658,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"3180f2db-54e6-47b3-f503-b83ba66e0eb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded text:  [36622, 416, 867, 517, 26, 329, 611, 345, 6758, 287, 428, 345, 481, 423, 8793, 645, 1402, 1943, 13, 447, 251, 554, 11982, 9550, 314, 16399, 284, 644, 616, 1545, 531, 11, 290, 465, 13050, 925, 884, 281, 10647, 319, 502, 326, 11, 1231, 9361, 284, 1808, 606, 11, 314, 6848, 511, 2128, 1108, 11, 290, 503, 286, 606, 314, 5295, 284, 787, 428, 3771, 2550, 26, 22881, 11, 10296, 9173, 11, 14210, 266, 2326, 19973, 616, 1545, 447, 247, 82, 922, 2565, 11, 616, 922, 15807, 287, 4917, 884, 281, 12534, 287, 884, 257, 640, 286, 761, 11, 290]\n","Decoded text:  sed by many more; for if you succeed in this you will have achieved no small success.” In profound silence I listened to what my friend said, and his observations made such an impression on me that, without attempting to question them, I admitted their soundness, and out of them I determined to make this Preface; wherein, gentle reader, thou wilt perceive my friend’s good sense, my good fortune in finding such an adviser in such a time of need, and\n","torch.Size([541159])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","encoded_text = enc.encode(text)\n","data = torch.tensor(encoded_text, dtype=torch.long)\n","\n","print('Encoded text: ', encoded_text[:100])\n","print('Decoded text: ', enc.decode(encoded_text[:100]))\n","print(data.shape)"]},{"cell_type":"code","execution_count":8,"id":"authorized-official","metadata":{"id":"authorized-official","executionInfo":{"status":"ok","timestamp":1699290522270,"user_tz":360,"elapsed":9,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["# Setting the training split to 90%\n","train_split = int(0.9 * len(data))\n","train_data = data[:train_split]\n","val_data = data[train_split:]"]},{"cell_type":"code","execution_count":9,"id":"listed-soundtrack","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"listed-soundtrack","executionInfo":{"status":"ok","timestamp":1699290522498,"user_tz":360,"elapsed":235,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"07795a0b-fc42-4254-86ac-3adee2271148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context:  tensor([36622]) Target:  tensor(416)\n","Context:  tensor([36622,   416]) Target:  tensor(867)\n","Context:  tensor([36622,   416,   867]) Target:  tensor(517)\n","Context:  tensor([36622,   416,   867,   517]) Target:  tensor(26)\n","Context:  tensor([36622,   416,   867,   517,    26]) Target:  tensor(329)\n","Context:  tensor([36622,   416,   867,   517,    26,   329]) Target:  tensor(611)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611]) Target:  tensor(345)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345]) Target:  tensor(6758)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758]) Target:  tensor(287)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287]) Target:  tensor(428)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428]) Target:  tensor(345)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345]) Target:  tensor(481)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481]) Target:  tensor(423)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423]) Target:  tensor(8793)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793]) Target:  tensor(645)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645]) Target:  tensor(1402)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402]) Target:  tensor(1943)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943]) Target:  tensor(13)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13]) Target:  tensor(447)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447]) Target:  tensor(251)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251]) Target:  tensor(554)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554]) Target:  tensor(11982)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982]) Target:  tensor(9550)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550]) Target:  tensor(314)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314]) Target:  tensor(16399)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399]) Target:  tensor(284)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284]) Target:  tensor(644)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644]) Target:  tensor(616)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616]) Target:  tensor(1545)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545]) Target:  tensor(531)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11]) Target:  tensor(290)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290]) Target:  tensor(465)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465]) Target:  tensor(13050)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050]) Target:  tensor(925)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925]) Target:  tensor(884)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884]) Target:  tensor(281)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281]) Target:  tensor(10647)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647]) Target:  tensor(319)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319]) Target:  tensor(502)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502]) Target:  tensor(326)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11]) Target:  tensor(1231)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231]) Target:  tensor(9361)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361]) Target:  tensor(284)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284]) Target:  tensor(1808)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808]) Target:  tensor(606)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11]) Target:  tensor(314)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314]) Target:  tensor(6848)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848]) Target:  tensor(511)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511]) Target:  tensor(2128)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128]) Target:  tensor(1108)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11]) Target:  tensor(290)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290]) Target:  tensor(503)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503]) Target:  tensor(286)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286]) Target:  tensor(606)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606]) Target:  tensor(314)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314]) Target:  tensor(5295)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295]) Target:  tensor(284)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284]) Target:  tensor(787)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787]) Target:  tensor(428)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428]) Target:  tensor(3771)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771]) Target:  tensor(2550)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550]) Target:  tensor(26)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26]) Target:  tensor(22881)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11]) Target:  tensor(10296)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296]) Target:  tensor(9173)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11]) Target:  tensor(14210)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210]) Target:  tensor(266)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266]) Target:  tensor(2326)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326]) Target:  tensor(19973)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973]) Target:  tensor(616)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616]) Target:  tensor(1545)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545]) Target:  tensor(447)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447]) Target:  tensor(247)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247]) Target:  tensor(82)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82]) Target:  tensor(922)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922]) Target:  tensor(2565)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11]) Target:  tensor(616)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616]) Target:  tensor(922)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922]) Target:  tensor(15807)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807]) Target:  tensor(287)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287]) Target:  tensor(4917)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917]) Target:  tensor(884)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884]) Target:  tensor(281)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281]) Target:  tensor(12534)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534]) Target:  tensor(287)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287]) Target:  tensor(884)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884]) Target:  tensor(257)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257]) Target:  tensor(640)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640]) Target:  tensor(286)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286]) Target:  tensor(761)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11]) Target:  tensor(290)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290]) Target:  tensor(644)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644]) Target:  tensor(14210)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210]) Target:  tensor(19338)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338]) Target:  tensor(8618)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618]) Target:  tensor(287)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287]) Target:  tensor(6464)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11]) Target:  tensor(1231)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231]) Target:  tensor(3090)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090]) Target:  tensor(393)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393]) Target:  tensor(35635)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11]) Target:  tensor(262)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262]) Target:  tensor(1621)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621]) Target:  tensor(286)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286]) Target:  tensor(262)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262]) Target:  tensor(5863)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863]) Target:  tensor(2094)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094]) Target:  tensor(2264)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264]) Target:  tensor(844)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844]) Target:  tensor(1258)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258]) Target:  tensor(286)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286]) Target:  tensor(4689)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689]) Target:  tensor(1869)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689,  1869]) Target:  tensor(11693)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689,  1869, 11693]) Target:  tensor(11)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689,  1869, 11693,    11]) Target:  tensor(508)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689,  1869, 11693,    11,   508]) Target:  tensor(318)\n","Context:  tensor([36622,   416,   867,   517,    26,   329,   611,   345,  6758,   287,\n","          428,   345,   481,   423,  8793,   645,  1402,  1943,    13,   447,\n","          251,   554, 11982,  9550,   314, 16399,   284,   644,   616,  1545,\n","          531,    11,   290,   465, 13050,   925,   884,   281, 10647,   319,\n","          502,   326,    11,  1231,  9361,   284,  1808,   606,    11,   314,\n","         6848,   511,  2128,  1108,    11,   290,   503,   286,   606,   314,\n","         5295,   284,   787,   428,  3771,  2550,    26, 22881,    11, 10296,\n","         9173,    11, 14210,   266,  2326, 19973,   616,  1545,   447,   247,\n","           82,   922,  2565,    11,   616,   922, 15807,   287,  4917,   884,\n","          281, 12534,   287,   884,   257,   640,   286,   761,    11,   290,\n","          644, 14210, 19338,  8618,   287,  6464,    11,  1231,  3090,   393,\n","        35635,    11,   262,  1621,   286,   262,  5863,  2094,  2264,   844,\n","         1258,   286,  4689,  1869, 11693,    11,   508,   318]) Target:  tensor(2714)\n"]}],"source":["block_size = 128\n","x, y = train_data[0: block_size], train_data[1: block_size + 1]\n","\n","for i in range(block_size):\n","    context= x[:i+1]\n","    target= y[i]\n","    print('Context: ', context, 'Target: ', target)"]},{"cell_type":"code","execution_count":10,"id":"verified-entry","metadata":{"id":"verified-entry","executionInfo":{"status":"ok","timestamp":1699290522498,"user_tz":360,"elapsed":3,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["def get_batch(data, seq_len, i):\n","    \"\"\"Get a batch of data\n","    Parameters\n","    ----------\n","    data : torch.tensor\n","        The data to get the batch from\n","    seq_len : int\n","        The length of the sequence\n","    i : int\n","        The batch index\n","    Returns\n","    -------\n","    torch.tensor\n","        The input sequence\n","    torch.tensor\n","        The target sequence\n","    \"\"\"\n","    seq_len = min(seq_len, len(data) - 1 - i)\n","    inputs = data[i: i + seq_len]\n","    targets = data[i + 1: i + 1 + seq_len].reshape(-1)\n","    return inputs, targets\n","\n","def get_random_batch(split_type='train', block_size=8, batch_size = 4):\n","    \"\"\"Get a random batch of data\n","    Parameters\n","    ----------\n","    split_type : str, optional\n","        The split to get the batch from, by default 'train'\n","    block_size : int, optional\n","        The size of the block (quantity of words per example in the batch), by default 8\n","    batch_size : int, optional\n","        The batch size (quantity of examples in the batch), by default 4\n","    Returns\n","    -------\n","    torch.tensor\n","        The input sequence of size (batch_size, block_size)\n","    torch.tensor\n","        The target sequence of size (batch_size, block_size)\n","    \"\"\"\n","    data = train_data if split_type == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","\n","    x, y = x.to(device), y.to(device)\n","\n","    return x, y\n","\n"]},{"cell_type":"code","execution_count":11,"id":"knowing-webcam","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knowing-webcam","executionInfo":{"status":"ok","timestamp":1699290529573,"user_tz":360,"elapsed":7077,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"d2d2c9b1-7eba-4d99-f09d-825d19cfdb91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch input x:   everything in its own way, so ordered\n","Size of complete X Batch:  torch.Size([4, 8])\n","Batch target y:   in its own way, so ordered it\n","Size of complete Y Batch:  torch.Size([4, 8])\n"]}],"source":["test_batch_x, test_batch_y= get_random_batch(split_type='train')\n","#printing the decoded batches\n","print('Batch input x: ', enc.decode(test_batch_x[0].tolist()))\n","print('Size of complete X Batch: ', test_batch_x.shape)\n","print('Batch target y: ', enc.decode(test_batch_y[0].tolist()))\n","print('Size of complete Y Batch: ', test_batch_y.shape)\n","\n","vocab_size = len(encoded_text)"]},{"cell_type":"markdown","id":"foreign-cyprus","metadata":{"id":"foreign-cyprus"},"source":["## Implementing Word2Vec Embeddings (can be changed later on like GloVe)\n","In this case, the embedding imported transforms each word and special character into a 300-dim vector, meaning that a sentence of 8 words will be transformed into a [8, 300] tensor."]},{"cell_type":"code","execution_count":12,"id":"periodic-muslim","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"periodic-muslim","executionInfo":{"status":"ok","timestamp":1699290551920,"user_tz":360,"elapsed":22356,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"4bf28e95-ca37-4004-9d4e-7e4ff44f06ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n","Collecting numpy==1.21.6\n","  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","jax 0.4.16 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","jaxlib 0.4.16+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","plotnine 0.12.3 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n","tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.21.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}],"source":["#installing past numpy version\n","!pip install gensim\n","!pip install numpy==1.21.6"]},{"cell_type":"code","execution_count":13,"id":"removable-seminar","metadata":{"id":"removable-seminar","executionInfo":{"status":"ok","timestamp":1699290627129,"user_tz":360,"elapsed":75220,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["#implementing word embedding using Word2Vec\n","from gensim.models import Word2Vec\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.manifold import TSNE\n","from gensim.models import KeyedVectors\n","\n","#loading model in './embedding_models/GoogleNews-vectors-negative300.bin'\n","model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/LLM TESTING/NanoGPT/embedding_models/GoogleNews-vectors-negative300.bin', binary=True)"]},{"cell_type":"code","execution_count":14,"id":"green-mathematics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"green-mathematics","executionInfo":{"status":"ok","timestamp":1699290628422,"user_tz":360,"elapsed":1308,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"97187a1a-e9ac-40ae-9d51-6dd872e4c1f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["(298727, 300)\n","Unique words (vocab size):  31700\n"]}],"source":["#using the model to get embeddings of our text\n","embeddings = []\n","for word in text.split(' '):\n","    try:\n","        embeddings.append(model[word])\n","    except:\n","        pass\n","\n","embeddings = np.array(embeddings)\n","print(embeddings.shape)\n","\n","words_vocab_size = len(list(set( text.split(' '))))\n","print(\"Unique words (vocab size): \", words_vocab_size)"]},{"cell_type":"code","execution_count":15,"id":"documented-arrival","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"documented-arrival","executionInfo":{"status":"ok","timestamp":1699290628423,"user_tz":360,"elapsed":14,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"be44d558-de61-46b6-f474-a308d5e85063"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word:  by\n","Embedding:  [-1.15722656e-01 -3.14941406e-02  1.59179688e-01  1.38671875e-01\n"," -5.06591797e-03  2.81982422e-02 -3.39355469e-02 -1.15722656e-01\n","  3.24707031e-02  1.38671875e-01 -1.00097656e-01 -1.26953125e-01\n"," -3.95507812e-02 -1.83105469e-02  2.34603882e-04 -8.05664062e-02\n","  4.51660156e-02  9.81445312e-02 -5.66406250e-02  6.00585938e-02\n","  8.88671875e-02  5.02929688e-02 -1.64794922e-02 -5.56640625e-02\n","  2.13867188e-01  1.42822266e-02 -1.31835938e-01  3.19824219e-02\n","  3.11279297e-02  1.25976562e-01 -3.47900391e-03 -1.55273438e-01\n"," -1.68945312e-01  1.40625000e-01  1.60156250e-01 -3.75976562e-02\n"," -9.47265625e-02 -3.90625000e-02  1.70898438e-01 -5.12695312e-02\n","  1.46484375e-01  9.61914062e-02 -2.66113281e-02  3.90625000e-02\n","  1.35742188e-01  5.27343750e-02 -4.15039062e-02 -5.20019531e-02\n","  1.57226562e-01  9.37500000e-02  5.67626953e-03  1.26953125e-01\n"," -5.31005859e-03 -4.80957031e-02  5.41992188e-02  1.66992188e-01\n","  3.85742188e-02  3.46679688e-02 -9.81445312e-02 -7.47070312e-02\n"," -1.33789062e-01  2.69775391e-02 -1.04003906e-01 -2.49023438e-02\n"," -7.71484375e-02 -2.36816406e-02 -1.22070312e-03  1.06445312e-01\n","  4.37011719e-02  1.96289062e-01 -2.20947266e-02 -7.95898438e-02\n","  2.49023438e-02  3.88183594e-02 -1.08032227e-02  2.26562500e-01\n","  8.69140625e-02  1.70898438e-01  5.07812500e-02  7.71484375e-02\n"," -1.30462646e-03  6.43920898e-03  4.17480469e-02  2.52685547e-02\n","  9.17968750e-02 -8.00781250e-02 -6.73828125e-02  4.00390625e-02\n"," -3.51562500e-02 -7.03125000e-02 -9.47265625e-02  1.81640625e-01\n"," -7.56835938e-02 -5.71289062e-02  2.87109375e-01  1.14257812e-01\n"," -1.46484375e-01 -1.61132812e-02 -5.70678711e-03 -4.32128906e-02\n","  2.18750000e-01 -1.07910156e-01 -5.15136719e-02  9.57031250e-02\n"," -4.02832031e-02  1.15722656e-01 -7.66601562e-02  4.78515625e-02\n","  2.80761719e-02 -1.04003906e-01  6.44531250e-02  4.85839844e-02\n"," -7.12890625e-02  3.27148438e-02 -6.73828125e-02 -8.15429688e-02\n","  4.37011719e-02  1.09863281e-03  1.70898438e-01  1.98242188e-01\n"," -1.23046875e-01  1.75781250e-01 -1.85546875e-01 -7.93457031e-03\n","  6.17675781e-02  3.34472656e-02  5.71289062e-02 -2.19726562e-03\n","  1.99218750e-01  6.54296875e-02 -2.82287598e-03  9.86328125e-02\n","  3.89099121e-03 -8.78906250e-02 -7.51953125e-02  2.52685547e-02\n"," -6.12792969e-02 -5.51757812e-02 -1.26953125e-01 -6.01196289e-03\n","  1.37695312e-01 -3.34472656e-02 -7.37304688e-02  7.81250000e-02\n","  4.17480469e-02 -5.93261719e-02  5.88378906e-02  9.26971436e-04\n","  8.64257812e-02  9.17968750e-02  5.39550781e-02  4.15039062e-02\n","  1.36718750e-01  1.89453125e-01  8.44726562e-02 -1.46484375e-01\n"," -5.88378906e-02 -1.66015625e-01  3.34472656e-02  5.21850586e-03\n"," -9.88769531e-03  8.30078125e-02  4.95605469e-02  1.06445312e-01\n"," -1.23046875e-01 -1.14135742e-02  1.07421875e-02 -7.47070312e-02\n"," -3.14941406e-02  1.23901367e-02 -1.46484375e-01 -2.07031250e-01\n"," -6.88476562e-02 -1.57226562e-01 -1.94335938e-01 -8.78906250e-02\n"," -4.54101562e-02  8.54492188e-02 -5.56640625e-02 -8.34960938e-02\n"," -5.12695312e-02 -1.64062500e-01 -1.17187500e-01  1.12792969e-01\n"," -6.05468750e-02  1.78222656e-02 -3.41796875e-02  1.05957031e-01\n","  8.39843750e-02  5.17578125e-02  6.05468750e-02  1.27563477e-02\n"," -1.01318359e-02  2.94189453e-02 -8.05664062e-02 -9.76562500e-02\n"," -4.79125977e-03  1.53198242e-02 -2.16064453e-02 -3.29589844e-02\n","  1.25000000e-01 -3.51562500e-02 -7.86132812e-02 -1.56250000e-02\n"," -1.20849609e-02  1.60156250e-01  6.64062500e-02  5.95703125e-02\n","  8.34960938e-02 -8.69140625e-02 -5.44433594e-02 -1.64062500e-01\n","  9.71679688e-02  1.01562500e-01  9.58251953e-03  2.35595703e-02\n","  8.39843750e-02 -4.19921875e-02 -4.07714844e-02  4.83398438e-02\n"," -2.46582031e-02 -6.78710938e-02  6.10351562e-02 -1.76757812e-01\n","  3.51562500e-02  4.71191406e-02  2.74658203e-02  1.40625000e-01\n","  2.67578125e-01 -5.12695312e-02  9.42382812e-02 -5.22460938e-02\n","  3.12500000e-02  3.36914062e-02 -8.74023438e-02 -9.08203125e-02\n","  1.03759766e-02  1.11328125e-01 -8.78906250e-02  1.01318359e-02\n"," -1.83593750e-01 -6.25000000e-02  2.01171875e-01 -8.91113281e-03\n","  2.39257812e-02  1.30859375e-01  3.12500000e-01  9.17968750e-02\n"," -1.04492188e-01  2.20947266e-02  5.83496094e-02 -1.44531250e-01\n"," -1.04492188e-01  9.81445312e-02  7.22656250e-02 -1.80664062e-01\n","  1.15722656e-01 -2.90527344e-02  8.64257812e-02 -9.08203125e-02\n","  7.51953125e-02  6.73828125e-02  1.55639648e-02  1.12792969e-01\n"," -1.20239258e-02  1.28906250e-01 -1.57470703e-02  1.38671875e-01\n","  1.73828125e-01  6.34765625e-03  7.37304688e-02  6.68945312e-02\n"," -1.19140625e-01  9.13085938e-02  1.74560547e-02 -8.88671875e-02\n"," -5.51757812e-02 -1.18652344e-01  1.23046875e-01  5.61523438e-03\n","  3.41796875e-02  1.45507812e-01 -5.83496094e-02 -4.19921875e-02\n","  1.57226562e-01  6.93359375e-02 -1.42578125e-01 -3.93066406e-02\n","  8.15429688e-02  6.03027344e-02 -1.21582031e-01 -7.47070312e-02\n"," -6.93359375e-02  3.51562500e-02 -3.22265625e-02  4.76074219e-02\n"," -2.12890625e-01  2.62451172e-02 -6.25000000e-02 -4.15039062e-02]\n"]}],"source":["#printing an example of the embedding\n","print('Word: ', text.split(' ')[1])\n","print('Embedding: ', embeddings[1])"]},{"cell_type":"code","execution_count":16,"id":"strange-termination","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":869},"id":"strange-termination","executionInfo":{"status":"ok","timestamp":1699290660551,"user_tz":360,"elapsed":32134,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"6b0bfbbe-13c7-4686-a342-e893e6a32d78"},"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x78c1a2f05b40>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 847, in match_library_callback\n","    self._make_controller_from_path(filepath)\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 984, in _make_controller_from_path\n","    lib_controller = controller_class(filepath=filepath, prefix=prefix)\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 111, in __init__\n","    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n","  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","OSError: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so: cannot open shared object file: No such file or directory\n"]},{"output_type":"stream","name":"stdout","text":["Reducing dimensionality...\n","Done!\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x1000 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["#using TSNE to reduce the dimensionality of the embeddings for just some words\n","print('Reducing dimensionality...')\n","tsne = TSNE(n_components=2)\n","embeddings_2d = tsne.fit_transform(embeddings[:2500])\n","print('Done!')\n","\n","#plotting the embeddings using matplotlib\n","plt.figure(figsize=(10, 10))\n","plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=1)\n","plt.show()"]},{"cell_type":"markdown","id":"fifth-confidentiality","metadata":{"id":"fifth-confidentiality"},"source":["## Making a Basic Bigram Model\n","Bigram models are the simplest form of language models that assigns probabilities to word sequences. It is based on the assumption that the probability of a word depends only on the previous word. In other words, it assumes that the probability of a word depends only on the previous word. The probability of a word depends on the previous two words in the case of a trigram model. The probability of a word depends on the previous n words in the case of an n-gram model.\n","\n","This simple model is just a random model that generates the next character based on the last character (not even the last n-characters). *Tokens DO NOT talk to each other*."]},{"cell_type":"code","execution_count":17,"id":"seven-numbers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seven-numbers","executionInfo":{"status":"ok","timestamp":1699290660552,"user_tz":360,"elapsed":7,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"671c8561-a3da-465d-857d-04a93b38319f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[71, 56, 70, 71, 60, 65, 58]\n","testing\n"]}],"source":["# For now, we will be using a basic encoding using only individual characters\n","\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"testing\"))\n","print(decode(encode(\"testing\")))"]},{"cell_type":"code","execution_count":18,"id":"broken-aluminum","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"broken-aluminum","executionInfo":{"status":"ok","timestamp":1699290660811,"user_tz":360,"elapsed":264,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"51623bab-e6de-482a-cd07-b713d3a81829"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2190617]) torch.int64\n","tensor([70, 56, 55,  0, 53, 76,  0, 64, 52, 65, 76,  0, 64, 66, 69, 56, 23,  0,\n","        57, 66, 69,  0, 60, 57,  0, 76, 66, 72,  0, 70, 72, 54, 54, 56, 56, 55,\n","         0, 60, 65,  0, 71, 59, 60, 70,  0, 76, 66, 72,  0, 74, 60, 63, 63,  0,\n","        59, 52, 73, 56,  0, 52, 54, 59, 60, 56, 73, 56, 55,  0, 65, 66,  0, 70,\n","        64, 52, 63, 63,  0, 70, 72, 54, 54, 56, 70, 70, 10, 97,  0, 33, 65,  0,\n","        67, 69, 66, 57, 66, 72, 65, 55,  0, 70, 60, 63, 56, 65, 54, 56,  0, 33,\n","         0, 63, 60, 70, 71, 56, 65, 56, 55,  0, 71, 66,  0, 74, 59, 52, 71,  0,\n","        64, 76,  0, 57, 69, 60, 56, 65, 55,  0, 70, 52, 60, 55,  8,  0, 52, 65,\n","        55,  0, 59, 60, 70,  0, 66, 53, 70, 56, 69, 73, 52, 71, 60, 66, 65, 70,\n","         0, 64, 52, 55, 56,  0, 70, 72, 54, 59,  0, 52, 65,  0, 60, 64, 67, 69,\n","        56, 70, 70, 60, 66, 65,  0, 66, 65,  0, 64, 56,  0, 71, 59, 52, 71,  8,\n","         0, 74, 60, 71, 59, 66, 72, 71,  0, 52, 71, 71, 56, 64, 67, 71, 60, 65,\n","        58,  0, 71, 66,  0, 68, 72, 56, 70, 71, 60, 66, 65,  0, 71, 59, 56, 64,\n","         8,  0, 33,  0, 52, 55, 64, 60, 71, 71, 56, 55,  0, 71, 59, 56, 60, 69,\n","         0, 70, 66, 72, 65, 55, 65, 56, 70, 70,  8,  0, 52, 65, 55,  0, 66, 72,\n","        71,  0, 66, 57,  0, 71, 59, 56, 64,  0, 33,  0, 55, 56, 71, 56, 69, 64,\n","        60, 65, 56, 55,  0, 71, 66,  0, 64, 52, 62, 56,  0, 71, 59, 60, 70,  0,\n","        40, 69, 56, 57, 52, 54, 56, 23,  0, 74, 59, 56, 69, 56, 60, 65,  8,  0,\n","        58, 56, 65, 71, 63, 56,  0, 69, 56, 52, 55, 56, 69,  8,  0, 71, 59, 66,\n","        72,  0, 74, 60, 63, 71,  0, 67, 56, 69, 54, 56, 60, 73, 56,  0, 64, 76,\n","         0, 57, 69, 60, 56, 65, 55, 95, 70,  0, 58, 66, 66, 55,  0, 70, 56, 65,\n","        70, 56,  8,  0, 64, 76,  0, 58, 66, 66, 55,  0, 57, 66, 69, 71, 72, 65,\n","        56,  0, 60, 65,  0, 57, 60, 65, 55, 60, 65, 58,  0, 70, 72, 54, 59,  0,\n","        52, 65,  0, 52, 55, 73, 60, 70, 56, 69,  0, 60, 65,  0, 70, 72, 54, 59,\n","         0, 52,  0, 71, 60, 64, 56,  0, 66, 57,  0, 65, 56, 56, 55,  8,  0, 52,\n","        65, 55,  0, 74, 59, 52, 71,  0, 71, 59, 66, 72,  0, 59, 52, 70, 71,  0,\n","        58, 52, 60, 65, 56, 55,  0, 60, 65,  0, 69, 56, 54, 56, 60, 73, 60, 65,\n","        58,  8,  0, 74, 60, 71, 59, 66, 72, 71,  0, 52, 55, 55])\n"]}],"source":["# let's now encode the entire text dataset and *store it into a torch.Tensor*\n","\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:500]) # the 500 characters we looked at earier will to the GPT look like this"]},{"cell_type":"code","execution_count":19,"id":"modern-spokesman","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"modern-spokesman","executionInfo":{"status":"ok","timestamp":1699290660811,"user_tz":360,"elapsed":6,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"6b5515b5-4092-45dc-9688-64132425866b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8])\n","torch.Size([4, 8])\n"]}],"source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","block_size= 8\n","batch_size = 4\n","x_train, y_train = get_random_batch(split_type='train', block_size=8, batch_size = 4)\n","\n","print(x_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":20,"id":"sorted-facility","metadata":{"id":"sorted-facility","executionInfo":{"status":"ok","timestamp":1699290660811,"user_tz":360,"elapsed":3,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed) # to generate embedding vectors that consider positio\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":27,"id":"complicated-liverpool","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"complicated-liverpool","executionInfo":{"status":"error","timestamp":1699290403988,"user_tz":360,"elapsed":559,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"6a38b133-f2d3-4f52-a820-c91141b441d8"},"outputs":[{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-ffe23c084f03>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logits: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-6a76c8c045ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# idx and targets are both (B,T) tensor of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtoken_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mposition_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtoken_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_emb\u001b[0m \u001b[0;31m#(B,T,C) #wont help much without se;f attention since this is just a BIgram model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B, T, vocal_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'T' referenced before assignment"]}],"source":["n_embed = 32\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","logits, loss= m(x_train.to(device), y_train.to(device))\n","print('Logits: ', logits.shape)\n","print(loss)\n"]},{"cell_type":"code","execution_count":null,"id":"operational-auckland","metadata":{"id":"operational-auckland","executionInfo":{"status":"aborted","timestamp":1699290403989,"user_tz":360,"elapsed":4,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["# Testing predictions\n","#Zero is going to be how we kick off the generation\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))"]},{"cell_type":"markdown","id":"beautiful-powder","metadata":{"id":"beautiful-powder"},"source":["## Adding an Optimizer to start Training"]},{"cell_type":"code","execution_count":null,"id":"temporal-sitting","metadata":{"id":"temporal-sitting","executionInfo":{"status":"aborted","timestamp":1699290403989,"user_tz":360,"elapsed":4,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-4)"]},{"cell_type":"code","execution_count":null,"id":"aware-disposal","metadata":{"id":"aware-disposal"},"outputs":[],"source":["for step in range(150000):\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","    if step % 10000 ==0:\n","        print(loss.item())\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"id":"weird-salon","metadata":{"id":"weird-salon"},"outputs":[],"source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=10000)[0].tolist()))"]},{"cell_type":"markdown","id":"turned-account","metadata":{"id":"turned-account"},"source":["## Visualizing a Basic Form of Self Attention\n","We'll make a barebones self-attention block that just averages the past characters of the input sequence. Since this is just a basic average, it will be significantly worse than a Transformer block or an RNN with LSTMs.\n","\n","A linear algebra trick to do cumulative sums is to use a triangular matrix."]},{"cell_type":"code","execution_count":null,"id":"first-engagement","metadata":{"id":"first-engagement"},"outputs":[],"source":["# version 1: using matrix multiply for a weighted aggregation\n","\n","B, T, C = batch_size, block_size, 2 #2 for easy visualization\n","x = torch.randn(B,T,C)\n","display(x)\n","\n","wei = torch.tril(torch.ones(T, T)) # square matrix of size of a single siquence on batch (block_size)\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x   #(BxTxT) . (BxTxC) ----> (BxTxC)\n","xbow2 #makes the \"cumulative\" average of past block, ddone separately for each sequencce in the bach (asuming embedding dim is 2)"]},{"cell_type":"code","execution_count":null,"id":"faced-persian","metadata":{"id":"faced-persian"},"outputs":[],"source":["# Adding softmax to pay attention to the 'main' word in the sequences\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf')) #for all elements where is 0, make it -inf\n","display(wei)\n","wei = F.softmax(wei, dim=-1)\n","display(wei) #since softmax is also like a normalization, you get the same matrix as before that would give us the average\n","xbow3 = wei @ x\n","display(xbow3) #note that it gives us the same, kjust anothe way to implement it.\n","#reason we end up using softmax is because the weights start as 0 and then how blocks from future wont be considered\n","#then it"]},{"cell_type":"code","execution_count":null,"id":"olympic-jones","metadata":{"id":"olympic-jones"},"outputs":[],"source":["# Addint on to the past code\n","B, T, C = batch_size, block_size, n_embed\n","x = torch.randn(B,T,C)\n","\n","#Adding one head of size 16 and initializing query, key and valuevectors\n","head_size= 16\n","key = nn.Linear(C, head_size, bias=0)\n","query = nn.Linear(C, head_size, bias=0)\n","value = nn.Linear(C, head_size, bias=0)\n","k = key(x) #(B, T, head_size)\n","q = query(x) #(B, T, head_size)\n","\n","\n","wei = q @ k.transpose(-2, -1) #we want to transpose the last two dimensions to make a dot product\n","#(B, T, 16) @ (B, 16, T) -> (B, T, T)\n","display(wei) #notice how the values are still not normalized and there it wwould still consier future blocks if masking was not implemented\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1) #exponentiate and normalize\n","display(wei)\n","\n","v = value(x)\n","out = wei @ v #averaging with all past blocks in sequence for each sequence\n","display(out.shape) #Now they have different attention importance\n"]},{"cell_type":"markdown","id":"liberal-brick","metadata":{"id":"liberal-brick"},"source":["## Implelmenting Attention\n"]},{"cell_type":"code","execution_count":null,"id":"round-portal","metadata":{"id":"round-portal"},"outputs":[],"source":["class SelfAttentionHead(nn.Module):\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embed, head_size, bias=0)\n","        self.query = nn.Linear(n_embed, head_size, bias=0)\n","        self.value = nn.Linear(n_embed, head_size, bias=0)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        self.dropout = nn.Dropout(dropout) # to randomly block some blocks from communicating with each other\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"manufactured-moore","metadata":{"id":"manufactured-moore"},"outputs":[],"source":["#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        self.sa_head = SelfAttentionHead(n_embed)\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        x= self.sa_head(x)\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"regional-harris","metadata":{"id":"regional-harris","outputId":"8eed1201-a916-42c1-d66f-a9fb90279e1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.47506856918335\n","2.158583641052246\n","2.4557456970214844\n","2.889756917953491\n","2.213761329650879\n","2.80466890335083\n","1.9321686029434204\n","2.3148622512817383\n","2.399808168411255\n","2.539961814880371\n"," Go st, ast, ho fofr is yso gr theminas n ililellle, prid sp irstho nt ond Samadtmerd ay bise nt ssatisend. Tharn I ins kt t isrod ed grnid tstegris ond istand the onim supnexteme satas o rind an ben omy hand dil, o, isen ache ny went g pal tre fel anto sa stom ge btous, stis havint poosry wilt beble angt the tis ed-or t.\" I therm teso-raventes th msi'and tved ay ower tm wigt he by shinid cher cthithim owsoupr haacry fuxomeret che and dd id Dughalll watnt ss, le ft th eit, ry te te ine chand thoru adve tere Reste fh.\"\" usst rltilelely tlexith I aend ld ws onesill awis lllove geanun unth nthanincul, I hatsaritheer ms ixicadulder said ulld the, a satw lf fagh dillland fay kle, rkn, pres hy I, HE,RTESLG X; ang minqu0udrot fys era ind, thew isred. \"S-cereend tporunte ng inosef fant mwin, totoerr ond yer met bhe I canidd th ghiele ple tllad aswisotoureertoum hussk beve od wey ym; Sbe sa th thany dithill thebed riald hagh, ist, amadimourer eardesisatred he ne nille shat ousut lte gnt ttsed ai\n"]}],"source":["n_embed = 32\n","dropout = 0.0\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(10000):\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","    if step % 1000 ==0:\n","        print(loss.item())\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"markdown","id":"altered-thermal","metadata":{"id":"altered-thermal"},"source":["## Adding MultiHead Attention\n","Basically creating multiple heads and running them in parallel. FInally, it concatenates their results."]},{"cell_type":"code","execution_count":null,"id":"narrow-chester","metadata":{"id":"narrow-chester"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        return out\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_random_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":null,"id":"aerial-reminder","metadata":{"id":"aerial-reminder"},"outputs":[],"source":["#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        x= self.sa_heads(x)\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"earned-ratio","metadata":{"id":"earned-ratio","outputId":"9621c20e-7e7d-477b-9137-3fc0057ecc77"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.008528 M parameters\n","step 0: train loss 4.4025, val loss 4.4057\n","step 200: train loss 2.8939, val loss 2.9933\n","step 400: train loss 2.7811, val loss 2.7781\n","step 600: train loss 2.6862, val loss 2.7078\n","step 800: train loss 2.6106, val loss 2.6319\n","step 1000: train loss 2.5679, val loss 2.6158\n","step 1200: train loss 2.4833, val loss 2.5567\n","step 1400: train loss 2.4758, val loss 2.5400\n","step 1600: train loss 2.4587, val loss 2.5012\n","step 1800: train loss 2.4379, val loss 2.4993\n","step 2000: train loss 2.4425, val loss 2.4896\n","step 2200: train loss 2.4647, val loss 2.4569\n","step 2400: train loss 2.4034, val loss 2.4578\n","step 2600: train loss 2.3885, val loss 2.4407\n","step 2800: train loss 2.3948, val loss 2.4828\n","step 3000: train loss 2.4002, val loss 2.4061\n","step 3200: train loss 2.3669, val loss 2.4239\n","step 3400: train loss 2.3630, val loss 2.3908\n","step 3600: train loss 2.3377, val loss 2.4158\n","step 3800: train loss 2.3648, val loss 2.4007\n","step 4000: train loss 2.3344, val loss 2.4054\n","step 4200: train loss 2.3553, val loss 2.3742\n","step 4400: train loss 2.3060, val loss 2.3763\n","step 4600: train loss 2.3245, val loss 2.3736\n","step 4800: train loss 2.3061, val loss 2.3362\n","step 5000: train loss 2.3012, val loss 2.3497\n","step 5200: train loss 2.3068, val loss 2.3360\n","step 5400: train loss 2.2988, val loss 2.3415\n","step 5600: train loss 2.2621, val loss 2.3315\n","step 5800: train loss 2.2597, val loss 2.3266\n","step 6000: train loss 2.2838, val loss 2.3413\n","step 6200: train loss 2.2487, val loss 2.3438\n","step 6400: train loss 2.2489, val loss 2.2890\n","step 6600: train loss 2.2469, val loss 2.3269\n","step 6800: train loss 2.2369, val loss 2.3135\n","step 7000: train loss 2.2565, val loss 2.2851\n","step 7200: train loss 2.2583, val loss 2.3027\n","step 7400: train loss 2.2313, val loss 2.3286\n","step 7600: train loss 2.2349, val loss 2.2917\n","step 7800: train loss 2.2335, val loss 2.3039\n","step 8000: train loss 2.2032, val loss 2.2865\n","step 8200: train loss 2.2021, val loss 2.2558\n","step 8400: train loss 2.2180, val loss 2.2796\n","step 8600: train loss 2.2125, val loss 2.2837\n","step 8800: train loss 2.2281, val loss 2.2990\n","step 9000: train loss 2.2201, val loss 2.2223\n","step 9200: train loss 2.2240, val loss 2.2754\n","step 9400: train loss 2.2214, val loss 2.2432\n","step 9600: train loss 2.1688, val loss 2.2881\n","step 9800: train loss 2.1816, val loss 2.2679\n","step 9999: train loss 2.2198, val loss 2.2521\n"," was hapavin. Wanchyo notme the thof adlien wome the cas igh high satto k SoES thead lay the up co duth ther, wilktiedy jof then gho I wis lo for, noor quit wissey mon thy coheaice,\" theit to the fore for, what.\" Sabr so s hiks, thad, athirting wothy toccon, the hon ingerd is the to so mith t to to throt of hat t for a t to acess helf ikne as ther meeardcot wadmy at trat if osed of thaco hast ho wa on as feess yho avaw ith, ha athought thoe thinem ifor OTH, THaraly colad hay, is, ato not Iag and ldy isereerseacthimpas aquieted may to nall mon re, wamthesth mary ghi hoom hi mas wheerses try ther dinoter vthe suas of his, ce B.\"xpy LABu frid vor omeborfin. Anot move Hcad Sel, me gonel, lexouon tis on co plemses eand o; \"cit sher.\"N\" OV Drrot As pep, herre the mend t the mer gore gag Tincchitoeer noto therach mor bron gorde a thafor'ng anoteny atharie it khillem;, U the ont,, dayll, acuin to is menot Doljdeand him matyesillribe the ghis ther, ap thim swilll Ipe hit tho ye nat theem him lab\n"]}],"source":["n_embed = 32\n","dropout = 0.0\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","max_iters = 10000\n","eval_iters = 200\n","\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if step % 200 == 0 or step == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"markdown","id":"closed-bosnia","metadata":{"id":"closed-bosnia"},"source":["# Adding a Feed Forward Layer"]},{"cell_type":"code","execution_count":null,"id":"hollywood-antibody","metadata":{"id":"hollywood-antibody"},"outputs":[],"source":["class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embed):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, n_embed),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"id":"cognitive-implement","metadata":{"id":"cognitive-implement"},"outputs":[],"source":["#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n","        self.ffwd = FeedFoward(n_embed)\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        x= self.sa_heads(x)\n","        x = self.ffwd(x)\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"authorized-beatles","metadata":{"id":"authorized-beatles"},"outputs":[],"source":["n_embed = 32\n","dropout = 0.0\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","max_iters = 10000\n","eval_iters = 200\n","\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if step % 200 == 0 or step == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"markdown","id":"precious-plaza","metadata":{"id":"precious-plaza"},"source":["## Adding a Transformer Block without the Cross-Self-Attention"]},{"cell_type":"code","execution_count":null,"id":"daily-customs","metadata":{"id":"daily-customs"},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embed, n_head):\n","        # n_embed: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embed // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embed)\n","\n","    def forward(self, x):\n","        x = self.sa(x)\n","        x = self.ffwd(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"id":"shaped-elder","metadata":{"id":"shaped-elder"},"outputs":[],"source":["#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        self.blocks = nn.Sequential(\n","            Transformer(n_embed, n_head = 8),\n","            Transformer(n_embed, n_head = 8),\n","            Transformer(n_embed, n_head = 8)\n","        )\n","\n","        #No logner needed because transformer does it\n","        #self.sa_heads = MultiHeadAttention(4, n_embed//4)\n","        #self.ffwd = FeedFoward(n_embed)\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        x = self.blocks(x)\n","        #x= self.sa_heads(x)\n","        #x = self.ffwd(x)\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"intended-gabriel","metadata":{"id":"intended-gabriel"},"outputs":[],"source":["n_embed = 32\n","dropout = 0.0\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","max_iters = 5000\n","eval_iters = 200\n","\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if step % 200 == 0 or step == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"markdown","id":"anticipated-invasion","metadata":{"id":"anticipated-invasion"},"source":["## Adding Residual Connections and Normalization to the Transformer Block\n","The model is becoming a Deep Neural Network, which suffer from optimization issues. To help on that, we can add the optimizations like Residual Networks and LayerNorm"]},{"cell_type":"code","execution_count":null,"id":"medieval-basin","metadata":{"id":"medieval-basin"},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embed, n_head):\n","        # n_embed: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embed // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embed)\n","\n","    def forward(self, x):\n","        # NEW\n","        x = x + self.sa(x)\n","        x = x + self.ffwd(x)\n","        return x\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n","        # NEW\n","        self.proj = nn.Linear(n_embed, n_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # NEW\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embed):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            # NEW\n","            nn.ReLU(),\n","            nn.Linear(4*n_embed, n_embed),\n","\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"id":"earned-extra","metadata":{"id":"earned-extra"},"outputs":[],"source":["#making a bigram model that uses the embeddings from our gensim model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, vocab_size, n_embed):\n","        \"\"\"This is the constructor of the class BigramModel.\n","        It initializes the embedding layer and the linear layer with the given parameters.\n","        the embedding layer will use the model from gensim to get the embeddings for the tokens.\"\"\"\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        self.blocks = nn.Sequential(\n","            Transformer(n_embed, n_head = 8),\n","            Transformer(n_embed, n_head = 8),\n","            Transformer(n_embed, n_head = 8)\n","        )\n","\n","        #No logner needed because transformer does it\n","        #self.sa_heads = MultiHeadAttention(4, n_embed//4)\n","        #self.ffwd = FeedFoward(n_embed)\n","        self.lm_head = nn.Linear(n_embed, vocab_size) #to do the embedding\n","\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(x) # (B,T,C)\n","        position_emb = self.position_embedding_table(torch.arange(T, device = device)) #(T,C)\n","        x= token_emb + position_emb #(B,T,C) #wont help much without se;f attention since this is just a BIgram model\n","        x = self.blocks(x)\n","        #x= self.sa_heads(x)\n","        #x = self.ffwd(x)\n","        logits = self.lm_head(x) #(B, T, vocal_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"introductory-encoding","metadata":{"id":"introductory-encoding","outputId":"c86a4c6c-1888-44e7-e1e5-61975e1dbc4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.04328 M parameters\n","step 0: train loss 4.7308, val loss 4.7388\n","step 200: train loss 2.6956, val loss 2.7654\n","step 400: train loss 2.5262, val loss 2.5936\n","step 600: train loss 2.5308, val loss 2.5747\n","step 800: train loss 2.4309, val loss 2.4410\n","step 1000: train loss 2.3735, val loss 2.4376\n","step 1200: train loss 2.3360, val loss 2.3839\n","step 1400: train loss 2.3287, val loss 2.4009\n","step 1600: train loss 2.3025, val loss 2.3126\n","step 1800: train loss 2.2777, val loss 2.3784\n","step 2000: train loss 2.2762, val loss 2.3070\n","step 2200: train loss 2.2509, val loss 2.2983\n","step 2400: train loss 2.2161, val loss 2.2590\n","step 2600: train loss 2.1700, val loss 2.2657\n","step 2800: train loss 2.1865, val loss 2.2771\n","step 3000: train loss 2.2415, val loss 2.2698\n","step 3200: train loss 2.1805, val loss 2.2360\n","step 3400: train loss 2.1833, val loss 2.2190\n","step 3600: train loss 2.1255, val loss 2.1919\n","step 3800: train loss 2.1510, val loss 2.1831\n","step 4000: train loss 2.1524, val loss 2.1708\n","step 4200: train loss 2.1246, val loss 2.1419\n","step 4400: train loss 2.1473, val loss 2.1949\n","step 4600: train loss 2.0834, val loss 2.1324\n","step 4800: train loss 2.0925, val loss 2.1900\n","step 4999: train loss 2.1060, val loss 2.1673\n"," panpestly bead wer it all he?y say diourse wereas I dicaicaoned roing ladsI for in the rey mabe bis he dothen sale whiche of mine, way in as oan e dy und'tand edelre be wailgt nioke ever, rrimed thad nreemeng in rowe thut te weridy whaty whe thou lriight my etrary.\" A go Ichiis of ot ghe but of Ting preeriste; bougsho, wicir har wko,\" senks hing of thoughtsy dreree cashimion; \"'houin guch to am of uto in cerrome aid sertsevidg lakrens to and alnothe a der ut der; the mifer firgrte,\" eveve sar eelarry I my brist,\" \"Waige shiree mote ring ans a.\" \"E DoNSIOPU'TENHAD OU dE I \"Ither; \"I him efSd ceeder the me liet, urlf it cayt herery thivartme; \"Bus yould wry dicaie! that and thich, and hims oit caid his daid yer I, weer and thouse way, forthery your war maay det freare wighther prearsess ose ivined this thereduibs I hips thale he endire. Quichte, rad and really weld roth the reasairts, rist withilimber and a yourd he her teld yereuris and ipell, and the wirk bu- and spaid, of searver le,'\n"]}],"source":["n_embed = 32\n","dropout = 0.0\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","max_iters = 5000\n","eval_iters = 200\n","\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if step % 200 == 0 or step == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    x_train, y_train = get_random_batch('train')\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"markdown","id":"secondary-silver","metadata":{"id":"secondary-silver"},"source":["## Adding a LayerNorm into Transformer Block\n","Is very similar to batch norm where we make sure that across the batch dimension we make sure that the data has unit gaussian distribution.\n","It changes the distribution of each token's embedding features into mean 0 and std 1."]},{"cell_type":"code","execution_count":21,"id":"enabling-headline","metadata":{"id":"enabling-headline","executionInfo":{"status":"ok","timestamp":1699290689446,"user_tz":360,"elapsed":175,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["class LayerNorm1d: # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n"]},{"cell_type":"code","execution_count":22,"id":"official-density","metadata":{"id":"official-density","executionInfo":{"status":"ok","timestamp":1699290690160,"user_tz":360,"elapsed":9,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_random_batch(split, block_size = block_size, batch_size= batch_size)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear( n_embed, head_size, bias=False)\n","        self.query = nn.Linear( n_embed, head_size, bias=False)\n","        self.value = nn.Linear( n_embed, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embed,  n_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        proj = self.proj(out)\n","        out = self.dropout(proj)\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self,  n_embed):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear( n_embed, 4 *  n_embed),\n","            nn.ReLU(),\n","            nn.Linear(4 *  n_embed,  n_embed),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Transformer(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self,  n_embed, n_head):\n","        #  n_embed: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size =  n_embed // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward( n_embed)\n","        self.ln1 = nn.LayerNorm( n_embed)\n","        self.ln2 = nn.LayerNorm( n_embed)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"code","execution_count":23,"id":"unlimited-gathering","metadata":{"id":"unlimited-gathering","executionInfo":{"status":"ok","timestamp":1699290690838,"user_tz":360,"elapsed":2,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"outputs":[],"source":["\n","class BigramModel(nn.Module):\n","\n","    def __init__(self, char_vocab_size, n_embed):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(char_vocab_size,  n_embed)\n","        self.position_embedding_table = nn.Embedding(block_size,  n_embed)\n","        self.blocks = nn.Sequential(*[Transformer( n_embed, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm( n_embed) # final layer norm\n","        self.lm_head = nn.Linear( n_embed, char_vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,char_vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"]},{"cell_type":"code","execution_count":24,"id":"wireless-guarantee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wireless-guarantee","executionInfo":{"status":"ok","timestamp":1699294708966,"user_tz":360,"elapsed":4017410,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}},"outputId":"07cfe0f0-648c-4fb5-90e5-a7729c4c72bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["10.815844 M parameters\n","step 0: train loss 4.8272, val loss 4.8244\n","step 200: train loss 2.3038, val loss 2.3594\n","step 400: train loss 1.8030, val loss 1.8779\n","step 600: train loss 1.5179, val loss 1.6204\n","step 800: train loss 1.3770, val loss 1.4992\n","step 1000: train loss 1.2948, val loss 1.4268\n","step 1200: train loss 1.2399, val loss 1.3841\n","step 1400: train loss 1.1937, val loss 1.3504\n","step 1600: train loss 1.1612, val loss 1.3162\n","step 1800: train loss 1.1361, val loss 1.2999\n","step 2000: train loss 1.1132, val loss 1.2861\n","step 2200: train loss 1.0921, val loss 1.2812\n","step 2400: train loss 1.0739, val loss 1.2652\n","step 2600: train loss 1.0598, val loss 1.2565\n","step 2800: train loss 1.0434, val loss 1.2516\n","step 3000: train loss 1.0314, val loss 1.2500\n","step 3200: train loss 1.0194, val loss 1.2397\n","step 3400: train loss 1.0085, val loss 1.2421\n","step 3600: train loss 0.9943, val loss 1.2435\n","step 3800: train loss 0.9827, val loss 1.2337\n","step 4000: train loss 0.9732, val loss 1.2320\n","step 4200: train loss 0.9651, val loss 1.2440\n","step 4400: train loss 0.9517, val loss 1.2394\n","step 4600: train loss 0.9444, val loss 1.2403\n","step 4800: train loss 0.9351, val loss 1.2303\n","step 4999: train loss 0.9252, val loss 1.2446\n"," seemed to the strick of luck. Sancho through a great slave. Glace enjoy To the brother of such an other joy and a touching of pure spinning in his senses. On this the head of his weak, Don Quixote said as high at the houseke, “Señor, poor, for all the truth, most a Christian, he who ears captain that your wants fail to take upon the point. The facult personing in which his praiseworthy to give off his request like times for his thumble beard; to fall in an age to enable him sore about, señora, that there was a blast watching the city of his son; or within what he perceives your worship was expecting in quest of your worship’s inscription; not to idea or old, understanding the other sorreness or fkiness; there was not very richly, for as soon insulted by the encouragements of entreating advice of seeing and record that she was bound in a rest, and until at the point that dines in rest, trustune, suspending oneselves to tell it that this so in a way in a morning among the village. It was\n"]}],"source":["# hyperparameters\n","batch_size = 64 # how many independent sequences will we process in parallel?\n","block_size = 256 # what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 100\n","learning_rate = 3e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embed = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2\n","\n","model = BigramModel(char_vocab_size, n_embed)\n","m = model.to(device)\n","\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n","\n","for step in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if step % 200 == 0 or step == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    x_train, y_train = get_random_batch('train', block_size = block_size, batch_size= batch_size)\n","    #print(x_train)\n","    logits, loss = m(x_train, y_train)\n","    optimizer.zero_grad(set_to_none = 1)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"code","source":["torch.save(m, '/content/drive/MyDrive/LLM TESTING/NanoGPT/model.pth')"],"metadata":{"id":"arZRh8BujZYY","executionInfo":{"status":"ok","timestamp":1699295598290,"user_tz":360,"elapsed":964,"user":{"displayName":"Álvaro López","userId":"12535611900818212285"}}},"id":"arZRh8BujZYY","execution_count":25,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BDsXCadk6Qyn"},"id":"BDsXCadk6Qyn","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}